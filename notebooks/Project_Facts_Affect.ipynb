{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import urllib3\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp @ Crawl\n",
    "datePosted = str(datetime.today())\n",
    "print('Time of Crawl: ' + datePosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do in future: Certificate verification, streaming\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) \n",
    "\n",
    "# FactCheck\n",
    "http = urllib3.PoolManager()\n",
    "url = 'https://www.factcheck.org/fake-news/'\n",
    "response = http.request('GET', url)\n",
    "soup = BeautifulSoup(response.data)\n",
    "\n",
    "# Get Last Page\n",
    "pages = [i.text for i in soup.find_all('a') if 'fake-news/page/' in str(i)]\n",
    "lastpage = pages[-1]\n",
    "\n",
    "# Actual Scrap\n",
    "adate = []\n",
    "description = []\n",
    "short_desc = []\n",
    "dataurl = []\n",
    "verdict = []\n",
    "\n",
    "for cp in np.arange(1,int(lastpage)+1):\n",
    "    url = 'https://www.factcheck.org/fake-news/page/' + str(cp)\n",
    "    response = http.request('GET', url)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    adate.append([i.text for i in soup.findAll('div', attrs={'class':'entry-meta'})])\n",
    "    # dataurl.append([i.get('data-url') for i in soup.findAll('div', attrs={'class':'at-above-post-cat-page addthis_tool', 'data-url': True})])\n",
    "    short_desc.append([i.text for i in soup.findAll('h3', attrs={'class':'entry-title'})])\n",
    "    description.append([i.p.text for i in soup.findAll('div', attrs={'class':'entry-content'})]) \n",
    "    urldiv = soup.findAll('div', attrs={'class':'col-12 col-sm-4'})\n",
    "    for div in urldiv:\n",
    "        dataurl.append(div.find('a')['href'])\n",
    "print('Initial Scrap Completed.')\n",
    "\n",
    "# Get Verdict Within Each Article's Embedded URL\n",
    "for link in dataurl:\n",
    "        response = http.request('GET', link)\n",
    "        soup = BeautifulSoup(response.data)\n",
    "        verdict.append([row.text for row in soup.findAll('div',attrs={'style':'font-weight: bold'})])\n",
    "print('Verdicts Retrieved.')\n",
    "\n",
    "# Flatten List [Output of Previous Section were Lists]\n",
    "descriptionflat = [y for x in description for y in x]\n",
    "short_descflat = [y for x in short_desc for y in x]\n",
    "dateflat = [y for x in adate for y in x]\n",
    "verdictflat = [y for x in verdict for y in x]\n",
    "formatdate = [datetime.strptime(re.sub('\\n', '', re.sub('\\t', '', a)), '%B %d, %Y') for a in dateflat]\n",
    "\n",
    "# CSV\n",
    "compiled_data = {'brief':short_descflat,'description':descriptionflat,'verdict':verdictflat,'date':formatdate,'url':dataurl}\n",
    "df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in compiled_data.items()]))\n",
    "df.to_csv('./Downloads/DI/FactCheck.csv',header=True,index=False)\n",
    "print('Saved as CSV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truth of Fiction\n",
    "\n",
    "url = 'https://www.truthorfiction.com/category/fact-checks/politics/'\n",
    "response = http.request('GET', url)\n",
    "soup = BeautifulSoup(response.data)\n",
    "\n",
    "# Get Last Page\n",
    "pages = [i.text for i in soup.find_all('a') if 'fact-checks/politics/page/' in str(i)]\n",
    "lastpage = pages[-2]\n",
    "\n",
    "# Actual Scrap\n",
    "adate = []\n",
    "description = []\n",
    "short_desc = []\n",
    "dataurl = []\n",
    "verdict = []\n",
    "alt = []\n",
    "\n",
    "for cp in np.arange(1,int(lastpage)+1):\n",
    "    tmpurl = []\n",
    "    url = 'https://www.truthorfiction.com/category/fact-checks/politics/' + str(cp)\n",
    "    response = http.request('GET', url)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    adate.append([i.text for i in soup.findAll('span', class_='tt-post-date')])\n",
    "    tmpurl.append([i.get('href') for i in soup.findAll('a', {'class':'tt-post-title c-h5', 'href':True})])\n",
    "    dataurl.append(tmpurl[0][:-1])\n",
    "    short_desc.append([i.text for i in soup.findAll('h3', attrs={'class':'entry-title'})])\n",
    "    description.append([i.p.text for i in soup.findAll('div', attrs={'class':'simple-text'})])  \n",
    "print('Initial Scrap Completed.')\n",
    "\n",
    "# Flatten List [Output of Previous Section were Lists]\n",
    "descriptionflat = [y for x in description for y in x]\n",
    "short_descflat = [y for x in short_desc for y in x]\n",
    "dateflat = [y for x in adate for y in x]\n",
    "dataurlflat = [y for x in dataurl for y in x]\n",
    "formatdate = [datetime.strptime(re.sub('\\n', '', re.sub('\\t', '', a)), '%B %d, %Y') for a in dateflat]\n",
    "\n",
    "# Get Verdict Within Each Article's Embedded URL\n",
    "for link in dataurlflat:\n",
    "    response = http.request('GET', link)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    try:\n",
    "        tmp = json.loads(soup.find_all('script', type='application/ld+json')[1].text)\n",
    "        verdict.append(tmp['reviewRating']['ratingValue'])\n",
    "        alt.append(tmp['reviewRating']['alternateName'])\n",
    "    except:\n",
    "        tmp = json.loads(soup.find_all('script', type='application/ld+json')[0].text)\n",
    "        verdict.append(tmp['reviewRating']['ratingValue'])\n",
    "        alt.append(tmp['reviewRating']['alternateName'])\n",
    "print('Verdicts Retrieved.')\n",
    "\n",
    "\n",
    "# CSV\n",
    "compiled_data = {'brief':short_descflat,'description':descriptionflat,'verdict':verdict,'altName':alt,'date':formatdate,'url':dataurlflat}\n",
    "df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in compiled_data.items()]))\n",
    "df.to_csv('./Downloads/DI/TorF.csv',header=True,index=False)\n",
    "print('Saved as CSV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Exploratory Analyses\n",
    "\n",
    "df1 = pd.read_csv('./Downloads/DI/FactCheck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('./Downloads/DI/users.csv')\n",
    "df4 = pd.read_csv('./Downloads/DI/tweets.csv')\n",
    "print(df3.head())\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4c = df4[df4['retweet_count'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.merge(df3,df4c,left_on='id',right_on='user_id')\n",
    "print(df_comb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets Per User\n",
    "\n",
    "# time_format ='%m/%d/%Y %H:%M:%S'\n",
    "time_format ='%Y/%m/%d %H:%M:%S'\n",
    "df_comb['created_str'] = pd.to_datetime(df_comb['created_str'],format = time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_comb\n",
    "df_test['tweet_ct'] = np.ones((len(df_test),1))\n",
    "df_test = df_test.set_index('created_str')\n",
    "# GB = df_test.groupby([(df_test.index.year),(df_test.index.month)]).sum()\n",
    "# GB.plot('abc','xyz',kind='scatter')\n",
    "# GB.index\n",
    "# seaborn.heatmap(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntweets = Counter(df_comb['id'])\n",
    "unpk = ntweets.most_common()[:-1]\n",
    "\n",
    "followers = []\n",
    "tweets = []\n",
    "for top in unpk:\n",
    "    fc = (df_comb.loc[df_comb['id'] == top[0],'followers_count'])\n",
    "    tc = top[1]\n",
    "    followers.append(fc.iloc[0])\n",
    "    tweets.append(tc)\n",
    "    \n",
    "df_work = pd.DataFrame({'Tweet Count': tweets, 'Follower Count':followers })\n",
    "df_work.dropna()\n",
    "\n",
    "plt.scatter(df_work['Tweet Count'], df_work['Follower Count'])\n",
    "df_work['Tweet Count'].corr(df_work['Follower Count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
