{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Recommendation Engines with PySpark\n",
    "\n",
    "##### This course was carried out on DataCamp using data stored on their AWS server (DataCamp Dataset, Movie Lens Subset, Million Songs Subset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col, array, explode, lit, struct\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample DataCamp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "ratings = spark.sql(\"SELECT * FROM example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correct format and assign IDs to distinct users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_long(df, by = [\"userId\"]): # \"by\" is the column by which you want the final output dataframe to be grouped by\n",
    "\n",
    "    cols = [c for c in df.columns if c not in by] \n",
    "    \n",
    "    kvs = explode(array([struct(lit(c).alias(\"movieId\"), col(c).alias(\"rating\")) for c in cols])).alias(\"kvs\") \n",
    "    \n",
    "    long_df = df.select(by + [kvs]).select(by + [\"kvs.movieId\", \"kvs.rating\"]).filter(\"rating IS NOT NULL\")\n",
    "    # Excluding null ratings values since ALS in Pyspark doesn't want blank/null values\n",
    "             \n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to the \"long\" format\n",
    "ratings = to_long(R)\n",
    "\n",
    "# Get unique users and repartition to 1 partition\n",
    "users = ratings.select(\"User\").distinct().coalesce(1)\n",
    "\n",
    "# Create a new column of unique integers called \"userId\" in the users dataframe\n",
    "users = users.withColumn(\"userID\", monotonically_increasing_id()).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign integer IDs to movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the distinct movie id's\n",
    "movies = ratings.select(\"Movie\").distinct() \n",
    "\n",
    "# Repartition the data to have only one partition\n",
    "movies = movies.coalesce(1) \n",
    "\n",
    "# Create a new column of movieId integers. \n",
    "movies = movies.withColumn(\"movieID\", monotonically_increasing_id()).persist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join ratings with userID and movieID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the ratings, users and movies dataframes\n",
    "movie_ratings = ratings.join(users, \"User\", \"left\").join(movies, \"Movie\", \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build an Alternating Least Squares (ALS) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ratings dataframe into training and test data\n",
    "(training_data, test_data) = ratings.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Set the ALS hyperparameters\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", rank =10, maxIter =15, regParam =.1,\n",
    "          coldStartStrategy=\"drop\", nonnegative =True, implicitPrefs = False)\n",
    "\n",
    "# Fit the model to the training_data\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data)\n",
    "\n",
    "# Build an evaluator using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Check that the 3 parameters have been appropriately set\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.getLabelCol())\n",
    "print(evaluator.getPredictionCol())\n",
    "\n",
    "# Evaluate the \"test_predictions\" dataframe\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print the RMSE\n",
    "print (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset of the MovieLens Dataset (Explicit Ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start with summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min num ratings for movies\n",
    "print(\"Movie with the fewest ratings: \")\n",
    "ratings.groupBy(\"movieId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per movie\n",
    "print(\"Avg num ratings per movie: \")\n",
    "ratings.groupBy(\"movieId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num ratings for user\n",
    "print(\"User with the fewest ratings: \")\n",
    "ratings.groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per users\n",
    "print(\"Avg num ratings per user: \")\n",
    "ratings.groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensure the data types are in the correct format for ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .printSchema() to see the datatypes of the ratings dataset\n",
    "ratings.printSchema()\n",
    "\n",
    "# Tell Spark to convert the columns to the proper data types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), ratings.movieId.cast(\"integer\"), ratings.rating.cast(\"double\"))\n",
    "\n",
    "# Call .printSchema() again to confirm the columns are now in the correct format\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up Grid Search and Evaluation of ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train set\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))\n",
    "\n",
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(train)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best_model\n",
    "print(type(best_model))\n",
    "\n",
    "# Complete the code below to extract the ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "\n",
    "# Print \"Rank\"\n",
    "print(\"  Rank:\", best_model.getRank())\n",
    "\n",
    "# Print \"MaxIter\"\n",
    "print(\"  MaxIter:\", best_model.getMaxIter())\n",
    "\n",
    "# Print \"RegParam\"\n",
    "print(\"  RegParam:\", best_model.getRegParam())\n",
    "\n",
    "# View the predictions \n",
    "test_predictions.show()\n",
    "\n",
    "# Calculate and print the RMSE of test_predictions\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up recommendation output\n",
    "\n",
    "ALS_recommendations.registerTempTable(\"ALS_recs_temp\")\n",
    "clean_recs = spark.sql(\"SELECT userId, movieIds_and_ratings.movieId AS movieId, movieIds_and_ratings.rating AS prediction\\\n",
    "                        FROM ALS_recs_temp\\\n",
    "                        LATERAL VIEW explode(recommendations) exploded_table\\\n",
    "                        AS movieIds_and_ratings\")\n",
    "\n",
    "# Explode Function\n",
    "exploded_recs = spark.sql(\"SELECT userId, explode(recommendations) AS MovieRec FROM ALS_recs_temp\")\n",
    "exploded_recs.show()\n",
    "\n",
    "# Adding lateral view\n",
    "ALS_recommendations.registerTempTable(\"ALS_recs_temp\")\n",
    "clean_recs = spark.sql(\"SELECT userId, movieIds_and_ratings.movieId AS movieId, movieIds_and_ratings.rating AS prediction \\\n",
    "                        FROM ALS_recs_temp \\\n",
    "                        LATERAL VIEW explode(recommendations) exploded_table \\\n",
    "                        AS movieIds_and_ratings\")\n",
    "\n",
    "# Explode and Lateral View together\n",
    "ALS_recommendations.registerTempTable(\"ALS_recs_temp\")\n",
    "clean_recs = spark.sql(\"SELECT userId, movieIds_and_ratings.movieId AS movieId, movieIds_and_ratings.rating AS prediction \\\n",
    "                        FROM ALS_recs_temp \\\n",
    "                        LATERAL VIEW explode(recommendations) exploded_table \\\n",
    "                        AS movieIds_and_ratings\")\n",
    "clean_recs.join(movie_info, [\"movieId\"],\"left\").show()\n",
    "\n",
    "# Filtering recommendations\n",
    "clean_recs.join(movie_ratings, [\"userId\",\"movieId\"],\"left\").show()\n",
    "\n",
    "# Filter by Null, i.e. Movies yet to be seen\n",
    "clean_recs.join(movie_ratings, [\"userId\",\"movieId\"],\"left\").filter(movie_ratings.rating.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset of the Million Songs Dataset (Implicit Ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start with summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "msd.show()\n",
    "\n",
    "# Count the number of distinct userIds\n",
    "user_count = msd.select(\"userId\").distinct().count()\n",
    "print(\"Number of users: \", user_count)\n",
    "\n",
    "# Count the number of distinct songIds\n",
    "song_count = msd.select(\"songId\").distinct().count()\n",
    "print(\"Number of songs: \", song_count)\n",
    "\n",
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]\n",
    "\n",
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"userId\", itemCol= \"songId\", ratingCol= \"num_plays\", rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The evaluation metric here will be Rank Order Error Metric (ROEM) which ALS does not include, so it will be custom coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROEM(predictions, userCol = \"userId\", itemCol = \"songId\", ratingCol = \"num_plays\"):\n",
    "    # Creates table that can be queried\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "\n",
    "    # Sum of total number of plays of all songs\n",
    "              denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "\n",
    "    # Calculating rankings of songs predictions by user\n",
    "    spark.sql(\"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "\n",
    "    # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "    numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n",
    "\n",
    "    performance = numerator/denominator\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(training, test) = msd.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build 5 folds within the training set\n",
    "train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "# Loops through all models and all folds\n",
    "for model in model_list:\n",
    "    for ft_pair in foldlist:\n",
    "\n",
    "        # Fits model to fold within training data\n",
    "        fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "        # Generates predictions using fitted_model on respective CV test data\n",
    "        predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "        # Generates and prints a ROEM metric CV test data\n",
    "        r = ROEM(predictions)\n",
    "        print (\"ROEM: \", r)\n",
    "\n",
    "    # Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(training)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "    # Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM: \", v_ROEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print winning model's index, model, and retrieve parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the smallest ROEM\n",
    "i = numpy.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", i)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[i])\n",
    "\n",
    "# Extract the best_model\n",
    "best_model = model_list[38]\n",
    "\n",
    "# Extract the Rank\n",
    "print (\"Rank: \", best_model.getRank())\n",
    "\n",
    "# Extract the MaxIter value\n",
    "print (\"MaxIter: \", best_model.getMaxIter())\n",
    "\n",
    "# Extract the RegParam value\n",
    "print (\"RegParam: \", best_model.getRegParam())\n",
    "\n",
    "# Extract the Alpha value\n",
    "print (\"Alpha: \", best_model.getAlpha())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
